{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW1_1a.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VwfKhFfHF27m","colab_type":"text"},"source":["# ML HW1 : wide hidden layer model"]},{"cell_type":"markdown","metadata":{"id":"qGeguvkAGNlw","colab_type":"text"},"source":["Mount google drive to colab."]},{"cell_type":"code","metadata":{"id":"RHc49hWDOJKw","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jly0ouf9eNOQ","colab_type":"text"},"source":["Import neccessary libraties and set parameters."]},{"cell_type":"code","metadata":{"id":"Qzie3tp6QThn","colab_type":"code","colab":{}},"source":["import os\n","import struct\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","mnist_path = \"http://yann.lecun.com/exdb/mnist/\"\n","\n","zip_list = [\"train-images-idx3-ubyte.gz\", \n","            \"train-labels-idx1-ubyte.gz\", \n","            \"t10k-images-idx3-ubyte.gz\", \n","            \"t10k-labels-idx1-ubyte.gz\" ]\n","\n","ext_list = [\"train-images.idx3-ubyte\", \n","            \"train-labels.idx1-ubyte\", \n","            \"t10k-images.idx3-ubyte\", \n","            \"t10k-labels.idx1-ubyte\" ]\n","\n","def downloadMNIST(path = \".\", unzip=True):\n","    import urllib.request\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","    for i in range(len(zip_list)):\n","        zip_name = zip_list[i]\n","        fname1 = os.path.join(path, zip_name)\n","        print(\"Download \", zip_name, \"...\")\n","        if not os.path.exists(fname1):\n","            urllib.request.urlretrieve( mnist_path + zip_name, fname1)\n","        else:\n","            print(\"pass\")\n","\n","        if unzip:\n","            import gzip\n","            import shutil\n","            ext_name = ext_list[i]\n","            fname2 = os.path.join(path, ext_name)\n","            print(\"Extract\", fname1, \"...\")\n","            if not os.path.exists(fname2):\n","                with gzip.open(fname1, 'rb') as f_in:\n","                    with open(fname2, 'wb') as f_out:\n","                        shutil.copyfileobj(f_in, f_out)\n","            else:\n","                print(\"pass\")\n","            \n","\n","def loadMNIST(dataset = \"training\", path = \".\"):\n","    if dataset == \"training\":\n","        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n","        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n","    elif dataset == \"testing\":\n","        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n","        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n","    else:\n","        raise ValueError(\"dataset must be 'testing' or 'training'\")\n","\n","    flbl = open(fname_lbl, 'rb')\n","    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n","    lbl = np.fromfile(flbl, dtype=np.int8)\n","    flbl.close()\n","\n","    fimg = open(fname_img, 'rb')\n","    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n","    img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows*cols)\n","    fimg.close()\n","\n","    return img, lbl\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1CqMp-7S05u","colab_type":"code","colab":{}},"source":["class NN(): \n","    def __init__(self, input_size, hidden_size, output_size, activation):   \n","        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n","        self.b1 = np.zeros([1, hidden_size])\n","        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(input_size)\n","        self.b2 = np.zeros([1, output_size])\n","        # Activation Function & Placeholders\n","        self.activation = activation # “linear”/“softmax”/”sigmoid”\n","        self.placeholder = {\"x\":None, \"y\":None}\n","\n","    \n","    # Feed Placeholder\n","    def feed(self, feed_dict):\n","        for key in feed_dict:\n","            self.placeholder[key] = feed_dict[key].copy()\n","    \n","    # Forward Propagation\n","    def relu(self, x):\n","        return np.maximum(x, 0)\n","    def forward(self):\n","        n = self.placeholder[\"x\"].shape[0]\n","        self.x = self.placeholder[\"x\"]\n","        self.B1 = np.ones((n, 1)).dot(self.b1)\n","        self.xw1 = self.x.dot(self.W1)\n","        self.a1 = self.xw1 + self.B1\n","        self.h1 = self.relu(self.a1)\n","        self.B2 = np.ones((n, 1)).dot(self.b2)\n","        self.xw2 = self.h1.dot(self.W2)\n","        self.a2 = self.xw2 + self.B2\n","\n","        if self.activation == \"linear\":\n","            self.y = self.a2.copy()\n","        # Softmax Activation\n","        elif self.activation == \"softmax\":\n","            self.y_logit = np.exp(self.a2 - np.max(self.a2, 1, keepdims=True))\n","            self.y = self.y_logit / np.sum(self.y_logit, 1, keepdims=True)\n","        # Sigmoid Activation\n","        elif self.activation == \"sigmoid\":\n","            self.y = 1.0 / (1.0 + np.exp(-self.a2))\n","        \n","        return self.y\n","    \n","    # Backward Propagation\n","    def backward(self):\n","        n = self.placeholder[\"y\"].shape[0]\n","        self.grad_a2 = (self.y - self.placeholder[\"y\"]) / n\n","        self.grad_b2 = np.ones((n, 1)).T.dot(self.grad_a2)\n","        self.grad_W2 = self.h1.T.dot(self.grad_a2)\n","        self.grad_h1 = self.grad_a2.dot(self.W2.T)\n","        self.grad_a1 = self.grad_h1.copy()\n","        self.grad_a1[self.a1 < 0] = 0\n","        self.grad_b1 = np.ones((n, 1)).T.dot(self.grad_a1)\n","        self.grad_W1 = self.x.T.dot(self.grad_a1)\n","    \n","    # Update Weights\n","    def update(self, learning_rate):\n","        self.W1 = self.W1 - learning_rate * self.grad_W1\n","        self.b1 = self.b1 - learning_rate * self.grad_b1\n","        self.W2 = self.W2 - learning_rate * self.grad_W2\n","        self.b2 = self.b2 - learning_rate * self.grad_b2\n","    \n","    # Loss Functions\n","    def computeLoss(self):\n","        loss = 0.0\n","        # Mean Square Error\n","        if self.activation == \"linear\":\n","            loss = 0.5 * np.square(self.y - self.placeholder[\"y\"]).mean()\n","        # Softmax Cross Entropy\n","        elif self.activation == \"softmax\":\n","            loss = -self.placeholder[\"y\"] * np.log(self.y + 1e-6)\n","            loss = np.sum(loss, 1).mean()\n","        # Sigmoid Cross Entropy\n","        elif self.activation == \"sigmoid\":\n","            loss = -self.placeholder[\"y\"] * np.log(self.y + 1e-6) - \\\n","            (1-self.placeholder[\"y\"]) * np.log(1-self.y + 1e-6)\n","            loss = np.mean(loss)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-NGM5m1S8sk","colab_type":"code","colab":{}},"source":["def OneHot(y):\n","    # todo (Digit Label to One-Hot Key)\n","    y_one_hot = np.eye(10, dtype=np.float32)[y]\n","    return y_one_hot\n","\n","def Accuracy(y,y_):\n","    # todo (Compute Accuracy)\n","    y_digit = np.argmax(y,1)\n","    y_digit_ = np.argmax(y_,1)\n","    temp = np.equal(y_digit, y_digit_).astype(np.float32)\n","    return np.sum(temp) / float(y_digit.shape[0])\n","\n","if __name__ == \"__main__\":\n","    # Dataset\n","    downloadMNIST(path='MNIST_data', unzip=True)\n","    x_train, y_train = loadMNIST(dataset=\"training\", path=\"MNIST_data\")\n","    x_test, y_test = loadMNIST(dataset=\"testing\", path=\"MNIST_data\")\n","\n","    # Show Data and Label\n","    plt.imshow(x_train[0].reshape((28,28)), cmap='gray')\n","    plt.show()\n","\n","    # todo (Data Processing)\n","    x_train = x_train.astype(np.float32) / 255.\n","    x_test = x_test.astype(np.float32) / 255.\n","    y_train = OneHot(y_train)\n","    y_test = OneHot(y_test)\n","\n","    # todo (Create NN Model)\n","    nn = NN(784, 256, 10, \"softmax\")\n","\n","    # Training the Model\n","    loss_rec = []\n","    batch_size = 64\n","    data_size = 60000\n","\n","    for i in range(10001):\n","        \n","        # todo (Sample Data Batch)\n","        batch_id = np.random.choice(data_size, batch_size)\n","        x_batch = x_train[batch_id]\n","        y_batch = y_train[batch_id]\n","        # todo (Forward & Backward & Update)\n","        nn.feed({\"x\":x_batch, \"y\":y_batch})\n","        nn.forward()\n","        nn.backward()\n","        nn.update(0.1)\n","\n","        # todo (Loss)\n","        loss = nn.computeLoss()\n","        loss_rec.append(loss)\n","\n","        # todo (Evaluation)\n","        batch_id = np.random.choice(x_test.shape[0], batch_size)\n","        x_test_batch = x_test[batch_id]\n","        y_test_batch = y_test[batch_id]\n","        nn.feed({\"x\":x_test_batch, \"y\":y_test_batch})\n","        y_test_out = nn.forward()\n","        acc = Accuracy(y_test_out, y_test_batch)\n","        if i%100 == 0:\n","            print(\"\\r[Iteration {:5d}] Loss={:.4f} | Acc={:.3f}\".format(i,loss,acc))\n","\n","    nn.feed({\"x\":x_test, \"y\":y_test})\n","    y_prob = nn.forward()\n","    total_acc = Accuracy(y_prob, y_test)\n","    print(\"Total Accuracy:\", total_acc)\n","\n","    plt.plot(loss_rec)\n","    plt.show()\n"],"execution_count":null,"outputs":[]}]}